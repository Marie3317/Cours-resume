{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca823ba-1a1b-479b-963c-831b7f7b823e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ad18d-2dd9-46b0-aa8a-7327cd08ef90",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4267869-780b-4549-9937-0ae2b047c6e7",
   "metadata": {},
   "source": [
    "Le ML est une famille d'algo qui apprenent à partir d'exemple.\n",
    "Il existe 4 catégories d'algo diff :\n",
    "    - régression : utilisé pour trouver un nb sur une dimension continue\n",
    "    - classification : utilisée pour trouver une catégorie\n",
    "    - supervisée : contient des données annotées afin que l'algo sache ce qu'il doit rechercher\n",
    "    - non-supervisé : contient des données qui ne sont pas annotées, et l'algo essaie de trouver des similitudes ou des diff pour faire ses propres regroupement dans les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44926d72-d1a0-408f-87d7-921e820f176d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Algo de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4463668f-77f2-4f79-acde-edcc093ab45e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classification KNN (K-nearest neighbors)/KNN Classifier    algo supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d642e9ef-3def-49f1-b5ba-81616050ac98",
   "metadata": {},
   "source": [
    "type d'algo de ML supervisé utilisé à la fois pour la classification et la régression.\n",
    "apprentissage supervisé : lorsque le dataset contient déjà une valeur cible y\n",
    "\n",
    "Dans le contexte de la classification, KNN est utilisé pour prédire la classe d'un échantillon en se basant sur les classes des échantillons voisins les plus proches. L'algorithme attribue la classe la plus fréquente parmi les k voisins les plus proches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e16d71-9c80-49a0-9696-fbec183f72bf",
   "metadata": {},
   "source": [
    "La différence entre KNeighborsClassifier et NearestNeighbors réside dans leur utilisation et leur fonctionnement.\n",
    "\n",
    "NearestNeighbors : Il s'agit d'une classe de la bibliothèque scikit-learn qui implémente l'algorithme des k-plus proches voisins (k-nearest neighbors) pour la recherche des voisins les plus proches. Cependant, NearestNeighbors est utilisé principalement pour la recherche de voisins sans étiquettes de classe spécifiques. Il peut être utilisé pour trouver les k voisins les plus proches d'un échantillon donné, mais il ne fournit pas de fonctionnalités de classification ou de prédiction. L'objectif principal de NearestNeighbors est d'identifier les échantillons similaires sans prendre en compte leurs étiquettes de classe.\n",
    "\n",
    "KNeighborsClassifier : Il s'agit également d'une classe de la bibliothèque scikit-learn qui implémente l'algorithme des k-plus proches voisins (k-nearest neighbors) mais spécifiquement pour la classification supervisée. KNeighborsClassifier est utilisé pour prédire les étiquettes de classe d'échantillons inconnus en se basant sur les étiquettes des k voisins les plus proches. Il prend en compte à la fois les caractéristiques (features) des échantillons et leurs étiquettes de classe pour effectuer la classification. KNeighborsClassifier est adapté aux problèmes de classification où vous avez des données d'entraînement étiquetées et souhaitez prédire l'étiquette de classe de nouveaux échantillons.\n",
    "\n",
    "En résumé, NearestNeighbors est utilisé pour trouver les voisins les plus proches d'un échantillon sans considération des étiquettes de classe, tandis que KNeighborsClassifier est utilisé pour la classification supervisée en se basant sur les étiquettes de classe des voisins les plus proches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b67d64-a603-4775-80e2-437f77b7dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b316957-ea12-46b0-9d98-03ca2c221e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5  # Nombre de voisins à considérer\n",
    "model = KNeighborsClassifier(n_neighbors = k)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fad09065-9c5b-4323-9a68-7a3f108ed377",
   "metadata": {},
   "source": [
    "Hyperparamètre :\n",
    "n_neighbors : C'est le nombre de voisins les plus proches à considérer lors de la prédiction. Il est important de choisir une valeur appropriée pour n_neighbors, car une valeur trop faible peut rendre le modèle sensible au bruit, tandis qu'une valeur trop élevée peut introduire du biais en agrégeant trop d'informations des voisins.\n",
    "\n",
    "weights : C'est la méthode utilisée pour pondérer l'influence des voisins. Par défaut, weights est défini sur 'uniform', ce qui signifie que tous les voisins contribuent de manière égale. Vous pouvez également utiliser 'distance', où les voisins sont pondérés en fonction de leur distance, de sorte que les voisins les plus proches ont une influence plus importante.\n",
    "\n",
    "metric : C'est la mesure de distance utilisée pour déterminer les voisins les plus proches. Par défaut, metric est défini sur 'minkowski' avec une valeur de p=2, ce qui correspond à la distance euclidienne. Vous pouvez choisir d'autres mesures de distance telles que 'manhattan' (distance de Manhattan) ou 'chebyshev' (distance de Chebyshev), ou même définir une fonction de distance personnalisée.\n",
    "\n",
    "algorithm : C'est l'algorithme utilisé pour trouver les voisins les plus proches. Par défaut, algorithm est défini sur 'auto', ce qui sélectionne l'algorithme approprié en fonction des données d'entrée. Vous pouvez également choisir 'brute' pour une recherche brute-force, 'kd_tree' pour une recherche basée sur l'arbre KD, ou 'ball_tree' pour une recherche basée sur l'arbre des boules.\n",
    "\n",
    "leaf_size : C'est la taille des feuilles utilisée dans les algorithmes basés sur l'arbre, tels que 'kd_tree' ou 'ball_tree'. Une valeur plus petite peut entraîner une meilleure précision, mais nécessite plus de temps de calcul."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a2e4c6-79b5-4898-be12-a0075b64bfc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LogisticRegression     algo supervisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dce86f-5622-4e3d-acef-baa20abad049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43107cad-46c8-4668-9c2d-b7162cc678b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5732ad8-2bd2-4115-867d-46aac5f22caf",
   "metadata": {},
   "source": [
    "La classe LogisticRegression de la bibliothèque scikit-learn (sklearn) propose différents hyperparamètres que vous pouvez régler lors de l'instanciation du modèle. Voici quelques-uns des hyperparamètres couramment utilisés pour la régression logistique :\n",
    "\n",
    "penalty : Cet hyperparamètre contrôle la régularisation appliquée lors de l'ajustement du modèle. Les valeurs courantes sont \"l1\" (régularisation L1 utilisant la norme L1) et \"l2\" (régularisation L2 utilisant la norme L2). Vous pouvez également spécifier \"none\" pour désactiver la régularisation. Par défaut, la valeur est \"l2\".\n",
    "\n",
    "C : C'est l'inverse de la force de régularisation. Une valeur plus petite de C donne une régularisation plus forte, tandis qu'une valeur plus grande donne une régularisation plus faible. Par défaut, C est égal à 1.0.\n",
    "\n",
    "solver : Cet hyperparamètre spécifie l'algorithme à utiliser pour l'optimisation du modèle. Les options courantes incluent \"lbfgs\" (BFGS à faible mémoire), \"liblinear\" (optimisation coordonnée avec descente de coordonnées), \"newton-cg\" (optimisation de la méthode de Newton-Conjugate Gradient), \"sag\" (optimisation de la moyenne stochastique) et \"saga\" (optimisation stochastique moyenne améliorée). Le choix de l'algorithme dépend de la taille des données et du problème spécifique.\n",
    "\n",
    "max_iter : C'est le nombre maximum d'itérations pour la convergence de l'optimisation. Par défaut, il est défini sur 100.\n",
    "\n",
    "Il existe d'autres hyperparamètres disponibles dans la classe LogisticRegression, mais ceux-ci sont parmi les plus couramment utilisés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3726e785-c54f-47c8-b516-a262eda3544c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DecisionTreeClassifier    algo supervisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae69fff-0f3e-482f-8cbb-fe3e2dfa082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6bdbe-0b19-47c8-8c7e-47c5822c6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du modèle de Decision Tree\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Entraînement du modèle sur les données d'entraînement\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e52735-8312-4579-a218-d4f1e5f4f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dessiner l'arbre\n",
    "plt.figure(figsize = (10,10))\n",
    "plot_tree(model, filled = True, class_names = model.classes_, feature_name = X_train.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abd21ec2-ec4a-443e-bdb6-fd0783e8db2f",
   "metadata": {},
   "source": [
    "Les hyperparamètres d'un modèle d'arbre de décision comprennent les paramètres qui contrôlent la construction de l'arbre et l'arrêt de la croissance de l'arbre. Voici quelques-uns des hyperparamètres les plus couramment utilisés :\n",
    "\n",
    "criterion : Le critère utilisé pour mesurer la qualité de la division d'un nœud. Les valeurs possibles sont \"gini\" (indice de Gini) et \"entropy\" (entropie).\n",
    "\n",
    "max_depth : La profondeur maximale de l'arbre. Cela limite le nombre de niveaux de décision de l'arbre.\n",
    "\n",
    "min_samples_split : Le nombre minimum d'échantillons requis pour diviser un nœud interne. Si le nombre d'échantillons dans un nœud est inférieur à cette valeur, la division ne sera pas effectuée.\n",
    "\n",
    "min_samples_leaf : Le nombre minimum d'échantillons requis dans une feuille. Si le nombre d'échantillons dans une feuille est inférieur à cette valeur, la division associée à ce nœud sera annulée.\n",
    "\n",
    "max_features : Le nombre maximum de caractéristiques à prendre en compte lors de la recherche de la meilleure division. Cela permet de contrôler la complexité de l'arbre.\n",
    "\n",
    "splitter : La stratégie utilisée pour choisir la division à chaque nœud. Les valeurs possibles sont \"best\" (meilleure division) et \"random\" (division aléatoire).\n",
    "\n",
    "random_state : La graine du générateur de nombres aléatoires utilisé pour l'initialisation aléatoire de l'arbre.\n",
    "\n",
    "Ce ne sont que quelques-uns des hyperparamètres disponibles pour un modèle d'arbre de décision. Selon la bibliothèque utilisée, il peut y avoir d'autres hyperparamètres spécifiques. Il est recommandé de consulter la documentation de la bibliothèque que vous utilisez pour obtenir une liste complète des hyperparamètres disponibles et leurs descriptions détaillées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04952e94-463c-42cb-9fce-7e4a9a4024b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Algo de régression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c5209-4767-4759-be5f-f95ee1c421c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### La régression linéaire simple/multiple   algo supervisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a054ab1-ce65-495e-b34a-019f373b088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22da55fa-c02d-47a6-bbbd-5f2fc64be8f6",
   "metadata": {},
   "source": [
    "son objectif est de trouver une ligne de régression\n",
    "il ne s'agit pas d'une causalité\n",
    "à nous de prendre du recul et de trouver si causalité ou corrélation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004c3a7-5c09-4618-9fc7-88b894973ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables explicatives\n",
    "X = df[[\"colonnes\"]]\n",
    "\n",
    "# Variable à expliquée\n",
    "y = df[[\"colonne\"]]\n",
    "\n",
    "# Créez une instance du modèle LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "# it's the \"a\" in your equation aX +b\n",
    "print(\"Coefficient :\", model.coef_)\n",
    "\n",
    "# it's the \"b\"\n",
    "print(\"Intercept :\", model.intercept_)\n",
    "\n",
    "# Regression linéaire multiple\n",
    "X = df[[\"colonne1\", \"colonne2\"...]]\n",
    "y = df[\"colonne\"]\n",
    "\n",
    "soit : print(\"Coeff :\", model.coef_)\n",
    "soit : for i, j in zip (X.columns, model.coef_):\n",
    "            print(i,j)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32cacd84-20e1-4816-84fc-d356cf56af1f",
   "metadata": {},
   "source": [
    "L'algorithme de régression linéaire, tel qu'implémenté dans la classe LinearRegression de la bibliothèque scikit-learn (sklearn), ne possède pas d'hyperparamètres spécifiques à régler lors de son instanciation. Cependant, il existe des paramètres qui peuvent être considérés comme des hyperparamètres lorsqu'ils sont utilisés en conjonction avec la régression linéaire. Voici certains de ces paramètres :\n",
    "\n",
    "fit_intercept : Ce paramètre contrôle si l'intercept (terme constant) doit être ajusté lors de l'ajustement du modèle. Par défaut, il est défini sur True, ce qui signifie que l'intercept est ajusté. Si vous souhaitez exclure l'intercept de votre modèle, vous pouvez le définir sur False.\n",
    "\n",
    "normalize : Ce paramètre spécifie si les variables d'entrée doivent être normalisées avant l'ajustement du modèle. S'il est défini sur True, les variables sont normées (centrées et réduites) avant l'ajustement. Par défaut, il est défini sur False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92adea5e-e87a-4633-8988-e7e992b8c47f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### KNN régressor     algo supervisé"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d33890cc-c34e-4945-b45e-abcc5a1a64f3",
   "metadata": {},
   "source": [
    "Dans le contexte de la régression, KNN est utilisé pour prédire une valeur continue en se basant sur les valeurs des k voisins les plus proches. L'algorithme calcule une valeur moyenne (ou une autre mesure de centralité) des cibles des échantillons voisins pour estimer la valeur de la cible pour un nouvel échantillon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a3fd75-446f-4324-aaef-6627e0822b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef2f17-3cb4-43a2-a580-d80f408fe005",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3  # Nombre de voisins\n",
    "model = KNeighborsRegressor(n_neighbors=k)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "382a4276-c68a-4ee6-861d-1966ffec3704",
   "metadata": {},
   "source": [
    "Les hyperparamètres les plus couramment utilisés pour l'algorithme KNeighborsRegressor (régression des k plus proches voisins) sont les suivants :\n",
    "\n",
    "n_neighbors : C'est le nombre de voisins à considérer lors de la prédiction. Il détermine combien de voisins les plus proches seront utilisés pour estimer la valeur d'une nouvelle instance. Une valeur plus élevée de n_neighbors donne une prédiction plus lissée, tandis qu'une valeur plus faible peut donner des prédictions plus sensibles aux points de données individuels.\n",
    "\n",
    "weights : C'est la méthode utilisée pour pondérer les contributions des voisins. Les options courantes sont :\n",
    "\n",
    "\"uniform\" : Tous les voisins ont un poids égal, c'est-à-dire qu'ils contribuent de manière uniforme à la prédiction.\n",
    "\n",
    "\"distance\" : Les voisins sont pondérés en fonction de leur distance par rapport à l'instance à prédire. Les voisins plus proches ont un poids plus important que les voisins plus éloignés.\n",
    "\n",
    "algorithm : C'est l'algorithme utilisé pour trouver les voisins les plus proches. Les options courantes sont :\n",
    "\n",
    "\"auto\" : L'algorithme choisit automatiquement l'algorithme le plus approprié en fonction des données d'entrée.\n",
    "\n",
    "\"ball_tree\" : Utilise une structure d'arbre pour trouver les voisins les plus proches.\n",
    "\n",
    "\"kd_tree\" : Utilise une structure d'arbre k-d pour trouver les voisins les plus proches.\n",
    "\n",
    "\"brute\" : Utilise une recherche brute-force pour trouver les voisins les plus proches. Cela peut être efficace pour de petites quantités de données.\n",
    "\n",
    "leaf_size : La taille de feuille utilisée dans les algorithmes \"ball_tree\" ou \"kd_tree\". Une valeur plus petite peut conduire à une meilleure précision, mais à un coût de calcul plus élevé.\n",
    "\n",
    "p : La norme de distance utilisée pour la mesure de la distance entre les points. La valeur par défaut est 2, ce qui correspond à la distance euclidienne. Une valeur de 1 correspond à la distance de Manhattan.\n",
    "\n",
    "Ce ne sont que quelques-uns des hyperparamètres les plus couramment utilisés pour KNeighborsRegressor. Il existe d'autres hyperparamètres spécifiques disponibles qui peuvent être explorés en fonction de vos besoins spécifiques.\n",
    "\n",
    "Lorsque vous entraînez un modèle KNeighborsRegressor, vous pouvez ajuster ces hyperparamètres en utilisant des techniques de validation croisée, de recherche en grille (grid search) ou d'optimisation bayésienne pour trouver la combinaison optimale qui offre les meilleures performances sur vos données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f90e72-6a90-4381-bcb1-152f8ee44bb2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### RegressorTree    algo supervisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c23ff-e639-4ae0-8d21-0f2bd3445298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du modèle de Decision Tree Regressor\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Entraînement du modèle sur les données d'entraînement\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40f0f2-5362-4f22-8639-bcc222051f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dessiner l'arbre:\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_tree(model, filled = True, feature_names = X_train.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0050c9f-c3d0-4bd2-9fb1-1a2962e426bc",
   "metadata": {},
   "source": [
    "Les hyperparamètres du modèle DecisionTreeRegressor comprennent les paramètres qui contrôlent la construction de l'arbre de décision et l'arrêt de la croissance de l'arbre. Voici quelques-uns des hyperparamètres les plus couramment utilisés :\n",
    "\n",
    "criterion : Le critère utilisé pour mesurer la qualité de la division d'un nœud. Les valeurs possibles sont \"mse\" (mean squared error) et \"friedman_mse\" (mean squared error basé sur la réduction de l'impureté de Friedman).\n",
    "\n",
    "max_depth : La profondeur maximale de l'arbre. Cela limite le nombre de niveaux de décision de l'arbre.\n",
    "\n",
    "min_samples_split : Le nombre minimum d'échantillons requis pour diviser un nœud interne. Si le nombre d'échantillons dans un nœud est inférieur à cette valeur, la division ne sera pas effectuée.\n",
    "\n",
    "min_samples_leaf : Le nombre minimum d'échantillons requis dans une feuille. Si le nombre d'échantillons dans une feuille est inférieur à cette valeur, la division associée à ce nœud sera annulée.\n",
    "\n",
    "max_features : Le nombre maximum de caractéristiques à prendre en compte lors de la recherche de la meilleure division. Cela permet de contrôler la complexité de l'arbre.\n",
    "\n",
    "splitter : La stratégie utilisée pour choisir la division à chaque nœud. Les valeurs possibles sont \"best\" (meilleure division) et \"random\" (division aléatoire).\n",
    "\n",
    "random_state : La graine du générateur de nombres aléatoires utilisé pour l'initialisation aléatoire de l'arbre.\n",
    "\n",
    "Ce ne sont que quelques-uns des hyperparamètres disponibles pour le modèle DecisionTreeRegressor. Selon la bibliothèque utilisée, il peut y avoir d'autres hyperparamètres spécifiques. Il est recommandé de consulter la documentation de la bibliothèque que vous utilisez pour obtenir une liste complète des hyperparamètres disponibles et leurs descriptions détaillées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03faba3-ec2a-42bf-aa26-de6e33d96037",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Autre : KNN NearestNeighbors    algo non supervisé"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8fb9c004-1f54-4abb-97be-7ec1126e9cc8",
   "metadata": {},
   "source": [
    "n'est ni un algorithme de classification ni un algorithme de régression. C'est un algorithme non supervisé utilisé pour la recherche des k-plus proches voisins (k-nearest neighbors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f270e55-94f2-4d2c-9b79-23181003a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15538873-1f02-4a28-b4dc-993500286bca",
   "metadata": {},
   "source": [
    "k = 2  # Nombre de voisins à trouver\n",
    "nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n",
    "\n",
    "# Recherche des k voisins les plus proches d'un df\n",
    "distances, indices = nbrs.kneighbors(df)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Indices des voisins les plus proches :\", indices)\n",
    "print(\"Distances vers les voisins les plus proches :\", distances)\n",
    "\n",
    "autre façon de faire :\n",
    "film_choisi = df_film_choisi.iloc[:, 4:]\n",
    "#création de la matrice pour rechercher les index des plus proches voisins\n",
    "matrice_des_plus_proches_voisins = distanceKNN.kneighbors(film_choisi)\n",
    "#création de la liste des suggestions à partir de la matrice\n",
    "suggestion = df_merge_finalML.iloc[matrice_des_plus_proches_voisins[1][0][1:], 1].values\n",
    "\n",
    "\n",
    "autre façon de faire :\n",
    "# on ne selectionne que les colonnes contenant des booleens sur la ligne du film choisi\n",
    "film_choisi = df_film_choisi.iloc[:, 8:]\n",
    "distance, indice = distanceKNN.kneighbors(film_choisi)\n",
    "pd.DataFrame(indice)  ou   pd.DataFrame(indice[0])   ou   indice[0,1:]\n",
    "#soit :\n",
    "print(\"On peut remplacer\", films, \"par\", df_merge_finalML.iloc[indice[0, 1:], 1].values)\n",
    "#soit :\n",
    "print(\"On peut remplacer\", films, \"par\", df_merge_finalML.iloc[indice[0, 1:]][\"primaryTitle\"].values)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09857aad-a413-4e0d-8a1d-1dc0333a8e35",
   "metadata": {},
   "source": [
    "La classe NearestNeighbors de la bibliothèque scikit-learn (sklearn) n'est pas utilisée pour la régression ou la classification, mais pour la recherche des k voisins les plus proches dans un ensemble de données non étiquetées. Par conséquent, elle n'a pas d'hyperparamètres spécifiques à la régression ou à la classification.\n",
    "\n",
    "Cependant, lors de l'instanciation de la classe NearestNeighbors, vous pouvez régler certains paramètres importants pour contrôler le comportement de l'algorithme, tels que :\n",
    "\n",
    "n_neighbors : C'est le nombre de voisins à rechercher. Il détermine combien de voisins les plus proches doivent être retournés pour chaque point de données. Une valeur plus élevée de n_neighbors donne plus de voisins proches, tandis qu'une valeur plus faible en donne moins.\n",
    "\n",
    "algorithm : C'est l'algorithme utilisé pour trouver les voisins les plus proches. Les options courantes sont \"auto\", \"ball_tree\", \"kd_tree\" et \"brute\". L'algorithme \"auto\" choisit automatiquement l'algorithme le plus approprié en fonction de la taille et de la dimension des données. \"ball_tree\" et \"kd_tree\" utilisent des structures d'arbre pour effectuer des recherches efficaces, tandis que \"brute\" effectue une recherche exhaustive par force brute. Le choix de l'algorithme dépend de la nature de vos données et des performances souhaitées.\n",
    "\n",
    "metric : C'est la mesure de distance utilisée pour évaluer la similarité entre les points de données. Les options courantes sont \"euclidean\" (distance euclidienne), \"manhattan\" (distance de Manhattan), \"chebyshev\" (distance de Chebyshev) et d'autres mesures de distance disponibles dans la bibliothèque scipy.spatial.distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e67af04-da70-4e07-ae39-59060d50a72e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Standardiser"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7644390-5e83-4a33-9bb6-825507c96cf0",
   "metadata": {},
   "source": [
    "Le scaler (ou normaliseur) est une étape de prétraitement des données dans le domaine de l'apprentissage automatique. Son objectif est de mettre à l'échelle les caractéristiques (variables) de manière cohérente avant de les utiliser dans un modèle d'apprentissage automatique. La mise à l'échelle des caractéristiques est souvent nécessaire car les caractéristiques peuvent avoir des échelles différentes, ce qui peut entraîner des problèmes lors de l'entraînement des modèles.\n",
    "\n",
    "Il existe différents types de scalers couramment utilisés, notamment :\n",
    "\n",
    "StandardScaler : Il transforme les caractéristiques en centrant les données autour de zéro (moyenne = 0) et en les mettant à l'échelle en divisant par l'écart type. Cela garantit que les caractéristiques ont une distribution gaussienne standard.\n",
    "\n",
    "MinMaxScaler : Il met à l'échelle les caractéristiques en les ramenant dans une plage spécifique, généralement entre 0 et 1. Il conserve la distribution des données et peut être utilisé lorsque vous souhaitez maintenir les valeurs relatives des caractéristiques.\n",
    "\n",
    "RobustScaler : Il est similaire au StandardScaler, mais utilise des mesures robustes aux valeurs aberrantes. Il est utile lorsque les données contiennent des valeurs extrêmes ou des valeurs aberrantes qui peuvent avoir un impact important sur la mise à l'échelle.\n",
    "\n",
    "L'utilisation du scaler dépend du type de données que vous traitez et du modèle que vous utilisez. Dans de nombreux cas, il est recommandé de mettre à l'échelle les caractéristiques avant d'entraîner un modèle, car cela peut améliorer la convergence et la performance du modèle. Cependant, il peut y avoir des cas où la mise à l'échelle n'est pas nécessaire, par exemple, lorsque vous utilisez des modèles basés sur les arbres de décision qui ne sont pas sensibles à l'échelle des caractéristiques.\n",
    "\n",
    "Il est important de noter que le scaler doit être ajusté sur les données d'entraînement uniquement, puis appliqué aux données de test ou de nouvelles données de la même manière pour maintenir la cohérence. Cela garantit que les caractéristiques sont mises à l'échelle de la même manière dans toutes les étapes du processus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e39837-9ebe-4d9e-91b3-4c673de7ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf215d1-5d4e-4010-bb8e-3a24832dca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ccd9f-e269-4961-8770-271cec3d7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1e94ae1-9b61-4c56-ade9-ad2cb70c2c32",
   "metadata": {},
   "source": [
    "Voici quelques considérations sur quand utiliser chaque option de mise à l'échelle :\n",
    "\n",
    "1. StandardScaler (normalisation standard) :\n",
    "   - Utilisez-le lorsque les caractéristiques doivent suivre une distribution gaussienne standard (moyenne = 0, écart type = 1).\n",
    "   - Convient aux algorithmes sensibles à l'échelle des caractéristiques, tels que la régression linéaire, la régression logistique, les SVM, les réseaux de neurones, etc.\n",
    "   - Utile lorsque vous voulez normaliser les caractéristiques sans perturber la structure des données.\n",
    "\n",
    "2. MinMaxScaler (normalisation min-max) :\n",
    "   - Utilisez-le lorsque vous souhaitez mettre les caractéristiques dans une plage spécifique (par exemple, entre 0 et 1).\n",
    "   - Convient aux algorithmes qui utilisent des distances ou des mesures basées sur les valeurs absolues, tels que les k-plus proches voisins (KNN) et les algorithmes basés sur la distance euclidienne.\n",
    "   - Utile lorsque vous voulez maintenir les valeurs relatives des caractéristiques.\n",
    "\n",
    "3. RobustScaler (normalisation robuste) :\n",
    "   - Utilisez-le lorsque vos données contiennent des valeurs aberrantes (valeurs extrêmes) qui peuvent fausser la mise à l'échelle.\n",
    "   - Convient aux situations où vous souhaitez une mise à l'échelle plus robuste aux valeurs aberrantes.\n",
    "   - Utile lorsque vous voulez une normalisation plus robuste qui est moins influencée par les valeurs extrêmes.\n",
    "\n",
    "En fin de compte, le choix du scaler dépend du contexte spécifique, des caractéristiques de vos données et du modèle d'apprentissage automatique que vous utilisez. Il est généralement recommandé d'expérimenter différents scalers et de comparer leurs effets sur les performances de votre modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5daa06-ed34-4a17-8355-b461c44ac335",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train, Test, Split"
   ]
  },
  {
   "cell_type": "raw",
   "id": "098fff8d-0394-4682-a857-a72683dde153",
   "metadata": {},
   "source": [
    "Diviser les données en 2 parties : un ensemble d'entraînement (75% à80% des données initiales) et un ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9037d267-97bc-402d-9b52-82308b0b67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2259bfc2-d5e4-43a1-a4c7-d8965f0394ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2da98afd-5894-4b43-a82d-97ac0bf80456",
   "metadata": {},
   "source": [
    "Hyperparamètres :\n",
    "    - random_state : En fixant une valeur pour random_state, vous garantissez que la division des données sera reproductible. Si random_state n'est pas spécifié, une division aléatoire sera effectuée à chaque appel à la fonction.\n",
    "    - train_size : Il s'agit de la taille (proportion) de l'ensemble d'entraînement par rapport à l'ensemble de données initial. Si train_size n'est pas spécifié, il est calculé automatiquement comme 1 - test_size. Vous pouvez utiliser train_size pour définir une proportion personnalisée.\n",
    "    - shuffle : Il s'agit d'un booléen indiquant si les données doivent être mélangées (permutées) avant d'effectuer la division. Par défaut, shuffle est défini sur True, ce qui signifie que les données seront mélangées avant la division. Si vous souhaitez conserver l'ordre original des données, vous pouvez définir shuffle sur False.\n",
    "    - stratify : Ce paramètre de stratification effectue une division de sorte que la proportion de valeurs dans l'échantillon produit soit la même que la proportion de valeurs fournies au paramètre de stratification. Très important dans la classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e214b-8664-41aa-b0f4-b989a341b3ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Entraînement du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e69ff5-e7e2-4e37-8b26-166e13d5af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînez le modèle avec les données d'entraînement\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a244af-6b3e-444f-b0ee-6ae42b0c0d59",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a7f7de-e79d-495d-bb2c-c4d8e97f58b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluer le model LR\n",
    "# avec le coeff de détermination R2 qui mesure la diff entre les valeurs prédites et réelles\n",
    "# 0 < R2 < 1\n",
    "# plus il tend vers 1 meilleur il est\n",
    "print(model.score(X,y))\n",
    "\n",
    "soit :\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "soit :\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Évaluation des performances du modèle\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# donne les proba de prédiction\n",
    "model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76472f7d-fcaa-4c16-8576-6a5cd5578700",
   "metadata": {},
   "source": [
    "NB : Le Mean Squared Error (MSE) est une mesure d'évaluation couramment utilisée pour évaluer les performances des modèles de régression. Il est applicable à tous les algorithmes de régression, y compris KNN (k-nearest neighbors), ainsi qu'à d'autres algorithmes tels que la régression linéaire, les arbres de décision, les réseaux neuronaux, etc.\n",
    "\n",
    "Le MSE est calculé en prenant la moyenne des carrés des différences entre les valeurs prédites par le modèle et les valeurs réelles. Il mesure la dispersion des prédictions par rapport aux vraies valeurs, et une valeur plus faible indique une meilleure performance du modèle. Cependant, le MSE est sensible aux valeurs aberrantes (outliers) car il prend en compte les carrés des différences.\n",
    "\n",
    "Il est important de noter que le choix de la métrique d'évaluation dépend du problème spécifique et des objectifs de l'analyse. Pour certains problèmes, d'autres métriques comme le coefficient de détermination R², l'erreur absolue moyenne (MAE) ou d'autres métriques peuvent être plus appropriées.\n",
    "\n",
    "En résumé, bien que le MSE soit souvent utilisé comme mesure d'évaluation pour les modèles de régression, il est recommandé de choisir la métrique d'évaluation en fonction du contexte spécifique et des caractéristiques du problème."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c7795-fbb0-4a68-8cf1-a6393156b19e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ratio de l'importance accordée à chq features pour la prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7163084-dfd9-4224-af62-0279dbd63cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.feature_importances_)\n",
    "\n",
    "# mise en relation et %\n",
    "importances = model.feature_importances_\n",
    "for feature, importance in (zip(X_train.columns, importances):\n",
    "            print(features, \":\", round(importance*100, 3), \"%\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d454fb26-9b66-4716-8fd5-1f06b3649c3e",
   "metadata": {},
   "source": [
    "la propriété feature_importances_ n'est pas applicable à tous les algorithmes de machine learning. Elle est spécifique aux algorithmes basés sur des arbres de décision, tels que Decision Tree, Random Forest, Gradient Boosting, etc.\n",
    "\n",
    "La propriété feature_importances_ renvoie un tableau ou une liste qui indique l'importance relative de chaque caractéristique (ou variable) dans le modèle entraîné. Elle est généralement utilisée pour évaluer l'importance des caractéristiques dans la prédiction du modèle.\n",
    "\n",
    "Cependant, tous les algorithmes de machine learning ne fournissent pas cette propriété. Par exemple, les modèles linéaires tels que la régression linéaire ou la régression logistique n'ont pas de feature_importances_ car ils n'utilisent pas de division de caractéristiques basée sur des arbres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b8b93-ed8e-4613-b82f-ac5226133572",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b325d3f-7331-4b6e-acc7-326b917bf2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faites des prédictions avec le modèle entraîné\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Affichez les valeurs prédites\n",
    "print(y_pred)\n",
    "\n",
    "# Afficher une prédiction de valeurs multiples\n",
    "print(model.predict(df[[\"colonne\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271a45b-6e9a-4733-bf4f-129d41337cfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Matrice de confusion    (que sur classification)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ab4a8eb-143c-4b7e-92f1-46707adb24a9",
   "metadata": {},
   "source": [
    "La matrice de confusion est une métrique couramment utilisée pour évaluer les performances d'un modèle de classification supervisée. Elle s'applique à tout algorithme de classification supervisée, qu'il s'agisse de KNN (k-nearest neighbors), de régression logistique, de SVM (Support Vector Machines), d'arbres de décision, de réseaux neuronaux ou d'autres algorithmes de classification.\n",
    "\n",
    "La matrice de confusion est une matrice carrée qui récapitule les prédictions du modèle par rapport aux étiquettes réelles (classes) des données. Elle présente le nombre de vrais positifs (TP), de vrais négatifs (TN), de faux positifs (FP) et de faux négatifs (FN). Ces valeurs permettent d'évaluer la performance du modèle en termes d'exactitude, de précision, de rappel, de spécificité et d'autres métriques.\n",
    "\n",
    "En résumé, la matrice de confusion est un outil d'évaluation largement utilisé pour analyser les performances des modèles de classification supervisée, quelle que soit la méthode d'apprentissage utilisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcac36a-2fc4-45ec-9c60-57ae49591b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16649be5-b021-477c-9883-655b89f0abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data = confusion_matrix(y_true = y_test, y_pred = model.predict(X_test)),\n",
    "             index = model.classes_ + \"ACTUAL\", columns = model.classes_ + \"PREDICTED\")\n",
    "\n",
    "peut aussi se faire sous forme d'array :\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "pour plus d'info :\n",
    "print(classification_report(y_test, model.predict(X_test)))\n",
    "\n",
    "viz avec seaborn et matplotlib\n",
    "sns.heatmap(conf_matrix, annot = True, fmt = \"g\")\n",
    "plt.ylabel(\"Prédict\", fontsize = 13)\n",
    "plt.xlabel(\"Actual\", fontsize = 13)\n",
    "plt.title(\"Confusion matrix\", fontsize = 17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ba1ea-8ded-4ea6-bdb0-54cb84363d09",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ca5d2-9875-4a85-a1b8-f539cfff9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "People at most risk :\n",
    "    a = model.predict_proba(X_test)\n",
    "    model.classes_  (affiche le nom des colonnes des probas)\n",
    "    survivant = a[:,1]  (on ne veut que la deuxième colonne)\n",
    "    df2 = pd.concat([X_test, y_test], axis = 1)\n",
    "    df2[\"nvelle_colonne\"] = survivant\n",
    "    df2 = df2.sort_values(by = \"nvlle_colonne\", ascending = False)\n",
    "\n",
    "Select 120 people with the highest proba of dying. Of these, how many actually survived ?\n",
    "    df3 = df_titanic[[\"name\", \"survived]]\n",
    "    df_titanic2= X.join(df3)\n",
    "    dead = df_titanic2.sort_values(by = \"proba_survie\", ascending = True).head(120)\n",
    "    dead.value_counts(\"survived\")\n",
    "                      \n",
    "Entre 2 personnes qui à la + grande chance de survie\n",
    "    df avec 2 lignes des personnes\n",
    "    model.predict_proba(df.iloc[:,:]) ou model.predict_proba(df)\n",
    "    model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4026c7-ad4f-48f6-9256-aaba7530a3ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CrossValidation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc139132-3c26-461b-a892-f51f552de788",
   "metadata": {},
   "source": [
    "Permet d'améliorer notre confiance dans la métrique utilisée, en évitant les biais liés au découpage du jeu d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0b125-8667-4d1b-a131-fbf0edb71024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34e135-74e9-4f73-8d82-9716f3b014b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(LogisticRegression(), X, y, cv = 3)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46bf273b-da04-43ee-ab8b-13b25a3da3e2",
   "metadata": {},
   "source": [
    "La validation croisée peut être utilisée avec la plupart des modèles de machine learning. Elle est une technique générale pour évaluer les performances d'un modèle et aider à estimer sa capacité à généraliser à de nouvelles données. Cependant, il est important de noter que certains modèles peuvent avoir des exigences particulières en ce qui concerne la validation croisée."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d75228f-d5e1-48c1-8962-919ceff79f97",
   "metadata": {},
   "source": [
    "En pratique, les valeurs couramment utilisées pour le nombre de folds sont 5 et 10. Cependant, vous n'êtes pas limité à ces choix et pouvez expérimenter avec différents nombres pour voir comment cela affecte les performances du modèle. L'important est de choisir un nombre de folds qui convient à votre ensemble de données et à vos objectifs d'évaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a7c318-461b-4944-ae58-5152b99d3c3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RandomSearch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56f0f3ca-bef5-48fb-bcd7-0a9ff230654b",
   "metadata": {},
   "source": [
    "RandomSearch est une technique d'optimisation hyperparamétrique utilisée pour rechercher de manière aléatoire l'espace des hyperparamètres d'un modèle d'apprentissage automatique. Les hyperparamètres sont des paramètres qui ne sont pas appris directement par le modèle, mais qui déterminent comment le modèle est entraîné et configuré.\n",
    "\n",
    "L'idée principale de RandomSearch est d'échantillonner aléatoirement différentes combinaisons d'hyperparamètres dans un espace de recherche prédéfini. Au lieu d'effectuer une recherche systématique et exhaustive de toutes les combinaisons possibles, RandomSearch explore un sous-ensemble aléatoire d'hyperparamètres, ce qui le rend plus efficace en termes de temps de calcul.\n",
    "\n",
    "Voici comment fonctionne RandomSearch :\n",
    "\n",
    "Définir l'espace de recherche : Pour chaque hyperparamètre du modèle, vous spécifiez une plage ou une distribution à partir de laquelle les valeurs seront échantillonnées. Par exemple, pour le taux d'apprentissage, vous pouvez définir une plage de valeurs entre 0,001 et 0,1.\n",
    "\n",
    "Spécifier le nombre d'itérations : Vous déterminez combien de combinaisons d'hyperparamètres vous souhaitez échantillonner dans l'espace de recherche. Plus le nombre d'itérations est élevé, plus RandomSearch explorera en profondeur l'espace des hyperparamètres, mais cela prendra également plus de temps.\n",
    "\n",
    "Échantillonnage aléatoire : À chaque itération, RandomSearch sélectionne aléatoirement une combinaison d'hyperparamètres dans l'espace de recherche défini. Cette combinaison est utilisée pour entraîner et évaluer le modèle.\n",
    "\n",
    "Évaluation des performances : Après chaque itération, le modèle est évalué en utilisant une métrique de performance spécifiée, telle que l'exactitude ou l'erreur quadratique moyenne. Cela permet de mesurer la qualité de la combinaison d'hyperparamètres actuellement évaluée.\n",
    "\n",
    "Sélection du meilleur ensemble d'hyperparamètres : Une fois que toutes les itérations sont terminées, RandomSearch identifie la combinaison d'hyperparamètres qui a donné les meilleures performances sur la métrique choisie. C'est cette combinaison qui est généralement sélectionnée comme le meilleur ensemble d'hyperparamètres pour le modèle.\n",
    "\n",
    "En utilisant RandomSearch, vous pouvez éviter de sélectionner manuellement les hyperparamètres et laisser l'algorithme explorer l'espace de recherche de manière aléatoire pour trouver une combinaison prometteuse. Cela facilite l'optimisation des performances du modèle en ajustant les hyperparamètres de manière efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470078b4-e819-445c-9af1-6c5dc779679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir l'estimateur (modèle)\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Définir l'espace de recherche pour les hyperparamètres\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Créer l'objet RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,  # Nombre d'itérations aléatoires\n",
    "    cv=5,  # Nombre de folds pour la validation croisée\n",
    "    scoring='accuracy'  # Métrique de performance à utiliser\n",
    ")\n",
    "\n",
    "# Effectuer la recherche aléatoire\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres et la meilleure performance\n",
    "print(\"Meilleurs hyperparamètres : \", random_search.best_params_)\n",
    "print(\"Meilleure performance : \", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55155c5c-582b-4696-be8e-d402fe4a87da",
   "metadata": {},
   "source": [
    "RandomSearch peut être utilisé avec la plupart des algorithmes de machine learning, quelle que soit la bibliothèque ou le framework que vous utilisez. Il n'est pas spécifiquement limité à un algorithme particulier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b4c23-b0a9-480d-a975-855b27765052",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GridSearch VS randomSearch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "082a9806-49e1-4c56-b832-75b0490bbd6a",
   "metadata": {},
   "source": [
    "La principale différence entre RandomSearch et GridSearch réside dans leur approche de recherche des hyperparamètres optimaux.\n",
    "\n",
    "Exploration de l'espace des hyperparamètres :\n",
    "\n",
    "RandomSearch : explore aléatoirement l'espace des hyperparamètres en échantillonnant différentes combinaisons d'hyperparamètres. Il sélectionne ces combinaisons de manière aléatoire, ce qui permet d'explorer un sous-ensemble diversifié de l'espace des hyperparamètres.\n",
    "GridSearch : effectue une recherche exhaustive dans l'espace des hyperparamètres en testant toutes les combinaisons possibles de valeurs spécifiées. Il divise l'espace des hyperparamètres en une grille régulière et évalue chaque combinaison sur cette grille.\n",
    "Efficacité de la recherche :\n",
    "\n",
    "RandomSearch : en échantillonnant aléatoirement des combinaisons d'hyperparamètres, RandomSearch est souvent plus rapide que GridSearch, car il explore seulement un sous-ensemble de l'espace des hyperparamètres.\n",
    "GridSearch : en testant toutes les combinaisons possibles, GridSearch peut être coûteux en termes de temps de calcul, surtout si l'espace des hyperparamètres est grand.\n",
    "Résultats obtenus :\n",
    "\n",
    "RandomSearch : en raison de son échantillonnage aléatoire, RandomSearch peut trouver de bonnes combinaisons d'hyperparamètres avec moins de recherche, mais il est moins susceptible de trouver la combinaison optimale.\n",
    "GridSearch : en effectuant une recherche exhaustive, GridSearch garantit de trouver la meilleure combinaison d'hyperparamètres dans l'espace de recherche spécifié. Cependant, cela peut entraîner une recherche plus lente et coûteuse.\n",
    "En résumé, RandomSearch est souvent préféré lorsque vous avez un espace de recherche d'hyperparamètres large et que vous voulez une recherche plus rapide et une bonne performance, même si la combinaison optimale n'est pas garantie. D'autre part, GridSearch est utile lorsque vous avez un espace de recherche relativement petit et que vous voulez trouver la meilleure combinaison d'hyperparamètres, même si cela nécessite une recherche exhaustive et peut prendre plus de temps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37591eaf-d751-4716-bc63-3c74c097f93c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31e09fc6-701d-4a8c-8fbb-702fd2988454",
   "metadata": {},
   "source": [
    "GridSearch est une technique d'optimisation hyperparamétrique utilisée pour rechercher de manière exhaustive l'espace des hyperparamètres d'un modèle d'apprentissage automatique. Son objectif principal est de trouver la meilleure combinaison d'hyperparamètres pour optimiser les performances du modèle.\n",
    "\n",
    "Voici à quoi sert GridSearch :\n",
    "\n",
    "Exploration exhaustive des hyperparamètres : GridSearch teste toutes les combinaisons possibles d'hyperparamètres spécifiées dans un espace de recherche prédéfini. Il divise l'espace des hyperparamètres en une grille régulière et évalue chaque combinaison sur cette grille. Cela garantit que toutes les combinaisons sont examinées, ce qui permet d'explorer de manière exhaustive l'espace des hyperparamètres.\n",
    "\n",
    "Identification des meilleurs hyperparamètres : En testant toutes les combinaisons, GridSearch permet d'identifier la combinaison d'hyperparamètres qui maximise les performances du modèle sur une métrique de performance spécifiée (par exemple, l'exactitude, l'erreur quadratique moyenne, etc.). Il fournit ainsi une solution optimale pour les hyperparamètres, ce qui permet d'obtenir les meilleurs résultats possibles.\n",
    "\n",
    "Comparaison des performances : GridSearch permet de comparer les performances du modèle pour différentes combinaisons d'hyperparamètres. Cela permet d'évaluer l'impact des différents hyperparamètres sur les performances du modèle et de prendre des décisions éclairées pour l'optimisation.\n",
    "\n",
    "Robustesse des résultats : En évaluant toutes les combinaisons, GridSearch fournit une solution robuste et reproductible, car il ne dépend pas de l'échantillonnage aléatoire des hyperparamètres. Cela garantit une recherche cohérente et permet de reproduire les résultats.\n",
    "\n",
    "GridSearch est particulièrement utile lorsque l'espace des hyperparamètres est relativement petit ou bien lorsque vous avez des contraintes strictes sur les performances du modèle. Il vous permet d'explorer de manière systématique toutes les combinaisons possibles, de trouver la meilleure configuration et d'optimiser les performances du modèle de manière exhaustive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de4af7-3657-4ad1-9f43-b18955769a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir l'estimateur (modèle)\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Définir l'espace de recherche pour les hyperparamètres\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Créer l'objet GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # Nombre de folds pour la validation croisée\n",
    "    scoring='accuracy'  # Métrique de performance à utiliser\n",
    ")\n",
    "\n",
    "# Effectuer la recherche sur la grille\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres et la meilleure performance\n",
    "print(\"Meilleurs hyperparamètres : \", grid_search.best_params_)\n",
    "print(\"Meilleure performance : \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "680a5600-9a8e-4d3e-aa33-90a68d1b9535",
   "metadata": {},
   "source": [
    "GridSearchCV de scikit-learn peut être utilisé avec la plupart des algorithmes de machine learning. Il n'est pas spécifiquement limité à un algorithme particulier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182f2ce-acf8-4dec-aade-fc48ac201b56",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pipeline PCA (Principal Component Analysis ou Analyse en Composantes Principales)    algo non supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41347b-44f6-4e3d-a45b-ab67e8bf3c4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PCA C'EST QUOI?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87b52491-1724-4935-a9da-767947d37dd1",
   "metadata": {},
   "source": [
    "Le pipeline PCA (Principal Component Analysis, en français Analyse en Composantes Principales) est une technique d'analyse de données utilisée pour réduire la dimensionnalité d'un ensemble de données. Il est utilisé pour identifier les variables les plus significatives ou les combinaisons linéaires de variables qui captent le plus d'informations dans les données.\n",
    "\n",
    "Voici une explication du pipeline PCA :\n",
    "\n",
    "Supposons que vous ayez un ensemble de données contenant plusieurs variables (ou features) qui décrivent différents aspects d'un objet ou d'un phénomène.\n",
    "\n",
    "Le pipeline PCA commence par centrer et réduire les données, ce qui signifie que les variables sont mises à une échelle commune et que leur moyenne est centrée sur zéro. Cela permet de prendre en compte les différentes échelles et d'éviter que certaines variables dominent les autres lors de l'analyse.\n",
    "\n",
    "Ensuite, le PCA calcule les composantes principales à partir des variables centrées et réduites. Les composantes principales sont de nouvelles variables qui sont des combinaisons linéaires des variables d'origine. La première composante principale capture la plus grande variation possible dans les données, la deuxième composante principale capture la deuxième plus grande variation, et ainsi de suite.\n",
    "\n",
    "Les composantes principales sont ordonnées en fonction de leur capacité à expliquer la variation dans les données. Les premières composantes principales sont les plus importantes, car elles capturent la majorité de l'information contenue dans les données.\n",
    "\n",
    "Vous pouvez choisir de conserver un certain nombre de composantes principales qui capturent une grande partie de la variation dans les données, tout en réduisant la dimensionnalité de l'ensemble de données. Cela permet de simplifier l'analyse, de réduire le bruit et d'éliminer les redondances.\n",
    "\n",
    "Enfin, vous pouvez utiliser les composantes principales sélectionnées comme nouvelles variables dans votre analyse. Elles peuvent être utilisées pour visualiser les données dans un espace de dimension inférieure, pour effectuer des analyses statistiques ou pour alimenter des modèles d'apprentissage automatique.\n",
    "\n",
    "QUAND L UTILISER ?\n",
    "Le PCA est couramment utilisé dans des domaines tels que la vision par ordinateur, la reconnaissance de formes, la compression de données, l'analyse de données et l'apprentissage automatique. Il est utile lorsque vous avez un grand nombre de variables dans votre ensemble de données et que vous souhaitez réduire la dimensionnalité tout en conservant une grande partie de l'information.\n",
    "\n",
    "Attention on perd de l information..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78200943-e30b-4c93-b20f-36e3131dfebe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## EXEMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e20caf-394f-4b22-b3bc-69a0cd03135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "892990f6-bc5a-485f-8003-7161f77816df",
   "metadata": {},
   "source": [
    "# Création du slice X avec les colonnes numériques\n",
    "X = df.select_dtypes(include='number')\n",
    "\n",
    "\n",
    "# Création du scaler et mise à l'échelle des données\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Instanciation et entraînement du modèle PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "\n",
    "# Calcul du nombre de composantes principales expliquant 70% et 80% de la variance\n",
    "variance_ratio_cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components_70 = np.argmax(variance_ratio_cumsum >= 0.7) + 1\n",
    "n_components_80 = np.argmax(variance_ratio_cumsum >= 0.8) + 1\n",
    "print(variance_ratio_cumsum)\n",
    "ou\n",
    "pca = PCA(n_components = 0.8)\n",
    "X_pca8 = pca.fit_transform(X_scaled)\n",
    "X_pca8.shape\n",
    "ou\n",
    "pca = PCA(n_components = 0.7)\n",
    "X_pca7 = pca.fit_transform(X_scaled)\n",
    "pca.explaindes_variance_ratio\n",
    "\n",
    "\n",
    "# Réduction de dimension avec le nombre de composantes sélectionné\n",
    "X_pca = pca.transform(X_scaled)\n",
    "\n",
    "\n",
    "# Sélection des deux premières composantes principales\n",
    "X_pca_2d = X_pca[:, :2]\n",
    "\n",
    "\n",
    "# Création du nuage de points\n",
    "plt.figure(figsize=(10, 8))\n",
    "for opinion in df['OPINION'].unique():\n",
    "    mask = df['OPINION'] == opinion\n",
    "    plt.scatter(X_pca_2d[mask, 0], X_pca_2d[mask, 1], label=opinion)\n",
    "\n",
    "# Ajout de légendes et de titres\n",
    "plt.xlabel('Composante Principale 1')\n",
    "plt.ylabel('Composante Principale 2')\n",
    "plt.title('Nuage de points des deux premières composantes principales')\n",
    "plt.legend()\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.show()\n",
    "\n",
    "#Lance un KNN sur X_scaled et y\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_scaled, y)\n",
    "model.score(X_scaled, y)\n",
    "\n",
    "#Lance un KNN avec les 2 premières colonnes X_pca et y\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "modelKNN_clr2 = KNeighborsClassifier()\n",
    "X_pca2 = X_pca[:,:2]\n",
    "modelKNN_clr2.fit(X_pca2, y)\n",
    "ConfusionMatrixDisplay(confusion_matrix(y, modelKNN_clr2.predict(X_pca2[:,:2]), labels = modelKNN_clr2.classes_), display_labels =modelKNN_clr2.classes_).plot();\n",
    "modelKNN_clr2.score(X_pca2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec3e7d-44b0-4e01-b11f-27cea384688d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ML Clustering"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c9faa88-cc24-4ce9-8c9f-c8a15f861f40",
   "metadata": {},
   "source": [
    "Le clustering est utilisé dans de nombreux domaines tels que la segmentation de marché, l'analyse des réseaux sociaux, la détection d'anomalies, la recommandation de produits, etc. Il peut aider à identifier des groupes similaires, à découvrir des schémas intéressants ou à effectuer une pré-analyse des données avant d'appliquer d'autres techniques d'apprentissage automatique\n",
    "\n",
    "Le clustering (ou regroupement) en apprentissage automatique (machine learning) est une technique qui consiste à regrouper un ensemble de données non étiquetées en sous-groupes homogènes ou similaires, appelés clusters. L'objectif du clustering est de découvrir des structures intrinsèques, des schémas ou des relations cachées dans les données sans avoir d'informations préalables sur les classes ou les étiquettes.\n",
    "\n",
    "En utilisant des algorithmes de clustering, le machine learning peut explorer et segmenter des ensembles de données en fonction de la similarité ou de la proximité entre les échantillons. Les algorithmes de clustering attribuent des étiquettes ou des identifiants de cluster à chaque échantillon, en veillant à ce que les échantillons au sein d'un même cluster soient similaires les uns aux autres, tandis que les échantillons de clusters différents sont différents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa749f0-5dda-47e8-9092-7691de9d789b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DBSCAN (Density-Based Spatial Clustering of Applications with Noise)     algo non supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869dc673-edcd-4aae-9739-ad1df14e59d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DBSCAN C'EST QUOI?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69d5a45c-cd69-4bcd-83c2-7da95dcaf9d7",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) est un algorithme de clustering basé sur la densité. Son objectif principal est de regrouper des datapoints dans des régions de haute densité tout en identifiant les régions de basse densité comme du bruit. Voici une définition simplifiée :\n",
    "\n",
    "DBSCAN identifie les clusters en recherchant des régions denses de datapoints dans l'espace des caractéristiques. Il commence par sélectionner un datapoint arbitraire et explore les datapoints voisins à l'aide d'un seuil de distance spécifié. S'il y a suffisamment de voisins à proximité, un nouveau cluster est formé. Ensuite, le processus est répété pour les voisins nouvellement ajoutés jusqu'à ce que tous les datapoints pertinents soient inclus dans le cluster. Si un datapoint est trop éloigné de tous les clusters existants, il est considéré comme du bruit.\n",
    "\n",
    "DBSCAN est utilisé dans les cas suivants :\n",
    "\n",
    "Données de densité variable : DBSCAN est efficace pour détecter des clusters dans des ensembles de données où la densité des datapoints peut varier. Il est capable de gérer des formes de cluster arbitraires et peut détecter les points aberrants qui ne font partie d'aucun cluster.\n",
    "\n",
    "Séparation des clusters : DBSCAN est capable de séparer naturellement les clusters même s'ils sont proches les uns des autres ou s'ils ont des formes irrégulières. Il est moins sensible à l'initialisation des clusters par rapport à des algorithmes tels que K-means.\n",
    "\n",
    "Grandes bases de données : DBSCAN est relativement efficace pour traiter de grandes bases de données en utilisant des structures de données telles que l'arbre kd-tree pour accélérer la recherche des voisins.\n",
    "\n",
    "Il convient de noter que DBSCAN nécessite de définir des paramètres tels que le rayon de voisinage et le nombre minimum de voisins pour qu'un point soit considéré comme dense. Ces paramètres peuvent varier en fonction des données et doivent être ajustés pour obtenir des résultats optimaux."
   ]
  },
  {
   "cell_type": "raw",
   "id": "90a44c8b-1dc8-4a35-98cc-87610685907b",
   "metadata": {},
   "source": [
    "Les valeurs -1 dans les prédictions de DBSCAN indiquent des points considérés comme des anomalies ou du bruit, c'est-à-dire des observations qui ne sont pas regroupées dans un cluster spécifique. Les autres valeurs entières positives (0, 1, 2, etc.) représentent les identifiants des clusters attribués à chaque observation.\n",
    "\n",
    "Dans la liste fournie, chaque élément correspond à la prédiction de cluster pour une observation spécifique dans l'ordre. Par exemple, la première observation a une prédiction de cluster de 0, la deuxième observation a également une prédiction de cluster de 0, et ainsi de suite.\n",
    "\n",
    "Il est important de noter que les identifiants des clusters attribués par DBSCAN n'ont pas de signification spécifique en termes de classe ou d'étiquette réelle. Les valeurs numériques des identifiants servent simplement à différencier les clusters les uns des autres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5700c92-a982-4a0c-83c9-c960a22442ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXEMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87003d08-51e7-4c05-ad97-e1401f1ad691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "256928e6-7b71-42c3-ad3a-4ca0f6638ea6",
   "metadata": {},
   "source": [
    "X = data.drop('species', axis=1)  # La variable X contient toutes les colonnes sauf 'species'\n",
    "Y = data['species']  # La variable Y contient la colonne 'species'\n",
    "\n",
    "# Création d'une figure et d'un axe\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Tracé des points de données pour chaque espèce avec des couleurs différentes\n",
    "# Tracé des points de données pour chaque espèce avec des couleurs différentes\n",
    "# Espèce 0 (setosa) en rouge\n",
    "ax.scatter(data[data['species'] == 0]['petal_length'], data[data['species'] == 0]['petal_width'], color='red', label='setosa')\n",
    "\n",
    "# Espèce 1 (versicolor) en bleu\n",
    "ax.scatter(data[data['species'] == 1]['petal_length'], data[data['species'] == 1]['petal_width'], color='blue', label='versicolor')\n",
    "\n",
    "# Espèce 2 (virginica) en vert\n",
    "ax.scatter(data[data['species'] == 2]['petal_length'], data[data['species'] == 2]['petal_width'], color='green', label='virginica')\n",
    "\n",
    "# Configuration des axes et du titre\n",
    "ax.set_xlabel('longueur du pétale')\n",
    "ax.set_ylabel('largeur du pétale')\n",
    "ax.set_title('Nuage de points des espèces d\\'iris')\n",
    "\n",
    "# Légende\n",
    "ax.legend()\n",
    "\n",
    "# Affichage du graphe\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "#Ce script utilise la méthode DBSCAN avec les paramètres par défaut pour regrouper les observations.\n",
    "#Les prédictions obtenues représentent le numéro du cluster assigné à chaque observation.\n",
    "\n",
    "# Création de l'objet DBSCAN avec les paramètres par défaut\n",
    "dbscan = DBSCAN()\n",
    "\n",
    "# Clustering des observations\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "# Affichage des prédictions pour chaque observation\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f79fcc1-6ab1-430e-a10a-562911df1cb7",
   "metadata": {},
   "source": [
    "#Ce script utilise la méthode DBSCAN avec les paramètres par défaut pour regrouper les observations.\n",
    "#Les prédictions obtenues représentent le numéro du cluster assigné à chaque observation.\n",
    "\n",
    "# Création de l'objet DBSCAN avec les paramètres par défaut\n",
    "dbscan = DBSCAN()\n",
    "\n",
    "# Clustering des observations\n",
    "labels = dbscan.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5c01bb-4196-4857-b5dc-4249b7bbf9ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyperparamètres"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bd349d1-3009-4242-a37d-9f31337e0540",
   "metadata": {},
   "source": [
    "Les paramètres eps et min_samples sont des hyperparamètres de l'algorithme DBSCAN (Density-Based Spatial Clustering of Applications with Noise). Voici leur explication :\n",
    "\n",
    "eps (epsilon) est la distance maximale entre deux points pour qu'ils soient considérés comme voisins. Cela définit la portée de la recherche de voisins pour chaque point. Les points qui se trouvent à une distance inférieure ou égale à eps seront regroupés ensemble. Une valeur plus grande d'eps permettra de regrouper plus de points, tandis qu'une valeur plus petite regroupera moins de points.\n",
    "min_samples est le nombre minimum de points requis pour former un cluster. Si le nombre de points dans un voisinage est inférieur à min_samples, alors ces points seront considérés comme du bruit (noise) plutôt que comme un cluster. Une valeur plus grande de min_samples permettra d'obtenir des clusters plus denses, tandis qu'une valeur plus petite permettra d'obtenir des clusters plus dispersés.\n",
    "Ainsi, en ajustant ces deux paramètres, vous pouvez contrôler la densité des clusters et la sensibilité aux points isolés (bruit)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b5a2d22-7df4-463c-9585-bc53d7142c02",
   "metadata": {},
   "source": [
    "Cependant, il est important de noter que dans cet exemple, nous avons utilisé uniquement deux variables (petal_length et petal_width) pour faciliter la visualisation. Dans des cas réels avec un plus grand nombre de variables, l'algorithme DBSCAN peut être capable de détecter des clusters plus complexes et de regrouper les observations de manière plus significative.\n",
    "\n",
    "De plus, l'analyse de clustering ne se limite pas seulement à la visualisation des clusters sur un graphique. Elle peut être utilisée pour l'exploration des données, l'identification des structures sous-jacentes, la détection d'anomalies, la segmentation de la population, etc. Par conséquent, bien que le graphique puisse sembler peu informatif dans ce cas particulier, il peut y avoir des insights intéressants à tirer de l'analyse des clusters avec DBSCAN dans des cas plus complexes ou avec un plus grand nombre de variables.\n",
    "\n",
    "Maintenant, j'espère que vous avez une compréhension solide de ce qu'est DBSCAN, comment l'implémenter avec Scikit-Learn et à quel point les différents hyperparamètres peuvent influencer votre algorithme de regroupement.\n",
    "\n",
    "Dans un monde idéal, il existerait un ensemble d'hyperparamètres qui fonctionnerait toujours le mieux pour chaque ensemble de données, mais malheureusement ce n'est pas le cas...\n",
    "\n",
    "Comme cela a été mentionné à plusieurs reprises, il convient de noter à nouveau que les algorithmes de regroupement sont généralement effectués en utilisant TOUS les attributs du jeu de données, mais dans cet exercice, nous n'en avons utilisé que 2 à des fins de visualisation facile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f745dcc6-4f86-4896-af9a-b92692447e23",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXEMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0f2f1-7ff9-490e-bd34-7ff7f11c527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783f552f-d7be-4bfe-a1fe-895fe7e2cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset.\n",
    "data = sns.load_dataset('iris')\n",
    "\n",
    "\n",
    "# For the 'species' column, replace [setosa, versicolor, virginica] with [0, 1, 2], respectively.\n",
    "# To be explicit, this means that all observations with a species of \"setosa\" should now have a species of 0\n",
    "data = data.replace({\"setosa\" : 0, \"versicolor\" : 1, \"virginica\" : 2})\n",
    "\n",
    "\n",
    "# Split the dataset into just X and Y\n",
    "# Note: Since we're clustering, there's no need to split into train/validation/test splits but rather just X and Y\n",
    "X = data[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "Y = data[\"species\"]\n",
    "\n",
    "\n",
    "# Using matplotlib, plot the original datapoints such that each species is a different color\n",
    "# Make petal_length the x-axis and petal_width the y-axis\n",
    "# For example, all observations with (species == 0) should be red, all observations with species == 1 should be blue, etc.\n",
    "\n",
    "# Create a scatter plot of petal_length vs petal_width\n",
    "fig, ax = plt.subplots()\n",
    "for i, species in enumerate(data['species'].unique()):\n",
    "    mask = data['species'] == species\n",
    "    ax.scatter(data[mask]['petal_length'], data[mask]['petal_width'], label=species)\n",
    "ax.legend()\n",
    "ax.set_xlabel('Petal Length')\n",
    "ax.set_ylabel('Petal Width')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Using the default parameters of DBSCAN in Scikit-Learn, cluster the observations and print out each observations predictions.\n",
    "# For example, the output should be something like [0, 0, 1, 1, 2, 2, ...]\n",
    "dbscan = DBSCAN()\n",
    "cluster_preds = dbscan.fit_predict(data)\n",
    "print(cluster_preds)\n",
    "\n",
    "\n",
    "\n",
    "# Using these predictions, again plot the predicted clusters with matplotlib.\n",
    "fig, ax = plt.subplots()\n",
    "for cluster in set(cluster_preds):\n",
    "    mask = cluster_preds == cluster\n",
    "    ax.scatter(data[mask]['petal_length'], data[mask]['petal_width'], label=f'Cluster {cluster}')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Petal Length')\n",
    "ax.set_ylabel('Petal Width')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Now choose epsilon to be 0.2 and minsamples to be 5 and repeat the process of predicting.\n",
    "# If you cannot recall what these hyperparameters represent, refer back to the Quest so that you have an understanding of what is changing\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "cluster_preds = dbscan.fit_predict(data)\n",
    "print(cluster_preds)\n",
    "\n",
    "\n",
    "# Question: Within these predictions, what do you think -1 represents? Remember that the only labels that we assigned we're 0, 1, and 2. \n",
    "# To give you a hint, what are the two things that DBSCAN is concerned with (which is also listed in the Quest)?\n",
    "# it's labeled as \"noise\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ec2d5a-2558-46fe-9b21-1fc92846fe00",
   "metadata": {
    "tags": []
   },
   "source": [
    "## K-means    algo non supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8693807-eb1f-4fca-b9ee-cbd404cb15a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### C'est quoi ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9996e9f-8e03-4e91-8bc5-6756b3a394a0",
   "metadata": {},
   "source": [
    "- algo permettant de regrouper les données\n",
    "- clustering méthode d'apprentissage non supervisé\n",
    "- sélectionne le nb de clusters que tu veux identifier dans ton data => c'est le K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9c6db-bd14-48d6-84b8-3dad828fef03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eee09ab-dbbf-42c7-bee7-f787253415d7",
   "metadata": {},
   "source": [
    "s'utilise sur des données quantitatives uniquement\n",
    "se base sur la distance multidimensionnel => normaliser les données avant de l'entraînner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4249f5-dca4-450c-ba73-24a10e1179af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a48a236-8e9f-4c5a-a26e-639c5d22ac40",
   "metadata": {},
   "source": [
    "modelKM = KMeans(n_clusters = 2)\n",
    "modelKM.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7db9d6-3620-4c47-81ed-5ecd1e5c38a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### centre des clusters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9793b2a-8604-42f2-be6e-5b8141dc917d",
   "metadata": {},
   "source": [
    "# l'initialisation du model est aléatoire. Suivant les entraînements on obtien un ordre différent\n",
    "modelKM.cluster_centers_\n",
    "\n",
    "# Initialisation non aléatoire\n",
    "modelKM = KMeans(n_clusters = 2, random_state = 3)\n",
    "modelKM.fit(X)\n",
    "\n",
    "# Cluster d'appartenance\n",
    "modelKM.labels_\n",
    "\n",
    "# Inertie : notion d'éparpillement des points. Un bon clustering à une inertie inter-classe grande (les clusters st biens séparés les uns des autres) et une inertie intra-classe petite (les pts à l'intérieur d'un cluster st denses).\n",
    "modelKM.inertia\n",
    "\n",
    "# Métrique : elbow et silhouette\n",
    "    elbow : méthode visuelle : on cherche le coude (elbow)\n",
    "    silhouette : chq point obtient un score entre -1 et 1\n",
    "                -1 : le pt est très mal placé (autre cluster?)\n",
    "                0 : hésitation ds le cluster d'appartenance\n",
    "                1 : le pt est au même endroit que le centre du cluster\n",
    "On calcule ensuite la moyenne de l'ensemble des scores. S'il est proche de 1, ts le pts st très proches de leurs clusters.\n",
    "from sklearn.metrics import silhouette_score\n",
    "for k in range (2, 9):\n",
    "    modelKM = KMeans(n_clusters = k, random_state = 3)\n",
    "    modelKM.fit(X)\n",
    "    print(silhouette_score(X, modelKM.labels_))\n",
    "On cherche ensuite le score le + élevé => nb optimal de clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ddd90-9b3c-48b7-9671-bf4836e5c930",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXEMPLES"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d446228d-985f-4dd3-b03a-00ad5a90c65b",
   "metadata": {},
   "source": [
    "# création de X contenant seulement les colonnes avec des données numériques\n",
    "X = df.select_dtypes(include='number')\n",
    "\n",
    "\n",
    "# Entraîne ton modèle pour qu’il trouve 3 clusters à partir des 4 colonnes numériques.\n",
    "modelKM = KMeans(n_clusters = 3)\n",
    "modelKM.fit(X)\n",
    "\n",
    "\n",
    "# Produis une visualisation comparant le dataset de base avec les classes générées par le K-means en t’inspirant de cette image. Tu peux prendre 2 dimensions de ton choix parmi les 4\n",
    "modelKM.cluster_centers_\n",
    "modelKM.labels_\n",
    "\n",
    "fig, (ax1, ax) = plt.subplots(ncols=2, figsize=(10,6))\n",
    "    # Graphique df initial\n",
    "sns.scatterplot(data=df, x='sepal_length', y='petal_width', hue='species', ax=ax)\n",
    "    # Graphique clusters\n",
    "cluster_labels = modelKM.labels_\n",
    "centroids = modelKM.cluster_centers_\n",
    "\n",
    "sns.scatterplot(data=X, x=X.iloc[:, 0], y= X.iloc[:, 3], hue=cluster_labels, ax=ax1)\n",
    "sns.scatterplot(x=centroids[:, 0], y= centroids[:, 3], marker='x', s=200, linewidths=5, color='g', ax=ax1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Évalue ton modèle en utilisant la méthode Elbow, entre 2 et 10 clusters, quel nombre cluster semble optimal ?\n",
    "inertia = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "    #visualize results\n",
    "plt.plot(range(2, 11), inertia)\n",
    "plt.xticks(range(2, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"inertia\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Évalue ton modèle en utilisant la méthode Silhouette, entre 2 et 10 clusters, quel nombre de clusters semble optimal ?pour un nombre de clusters entre 1 et 10 ?\n",
    "silhouette = []\n",
    "for k in range(2, 11):\n",
    "    kmeans2 = KMeans(n_clusters=k)\n",
    "    kmeans2.fit(X)\n",
    "    silhouette.append(silhouette_score(X, kmeans2.labels_))\n",
    "\n",
    "    #visualize results\n",
    "plt.plot(range(2, 11), silhouette)\n",
    "plt.xticks(range(2, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe4acca-2031-40d1-a4f6-21c9bb6fd892",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ce1fb1-b5db-49bf-b90e-ea005df18795",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Def"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a6bc0e5-976b-4fce-a696-7a8f4f598754",
   "metadata": {},
   "source": [
    "L'un des principaux inconvénients des autres algorithmes de clustering (tels que K-Means par exemple) est que vous devez spécifier au préalable le nombre de clusters distincts que vous souhaitez. Dans le clustering hiérarchique, cependant, ce n'est pas nécessairement le cas ! En réalité, en utilisant le clustering hiérarchique, vous obtenez une représentation visuelle arborescente des observations, qui dans le monde de l'apprentissage automatique sont appelées dendogrammes. Étant donné que nous n'avons pas besoin de spécifier la quantité de clusters et que nous commençons plutôt par construire les clusters à partir des observations vers le haut, cette approche est communément appelée approche ascendante du clustering."
   ]
  },
  {
   "cell_type": "raw",
   "id": "793e442b-cdbe-4ec5-a50f-6fd7a9ad7df1",
   "metadata": {},
   "source": [
    "Liaison complète : Étant donné deux clusters avec plusieurs points dans chaque cluster, mesurez la distance par paires entre tous les points du cluster A et tous les points du cluster B. Quelle que soit la distance entre deux points la plus grande, choisissez celle-ci comme étant la distance entre le cluster A et le cluster B.\n",
    "\n",
    "Liaison simple : similaire à la liaison complète, mesurez d'abord la distance par paires entre tous les points du groupe A et tous les points du groupe B. Plutôt que de prendre la plus grande distance par paires, cependant, nous choisirons la ***plus petite ***distance entre deux points comme distance entre le cluster A et le cluster B.\n",
    "\n",
    "Liaison moyenne : au lieu de prendre la distance la plus grande ou la plus petite lors de la mesure de la distance par paires, calculez la ***moyenne *** de chacune de ces distances entre tous les points du groupe A et tous les points du groupe B.\n",
    "\n",
    "Liaison centroïde : trouvez la distance entre le *** centroïde *** du cluster A et le centroïde du cluster B. Le centroïde d'un seul cluster est considéré comme le milieu du cluster par rapport aux points de données de cette classe. Par exemple, dans un sens unidimensionnel, disons que Cluster A = [0, 1, 5, 6, 9]. Le centre de gravité serait alors 4,25. Maintenant, en faisant la même chose avec le cluster B, la différence entre ces deux centroïdes sera utilisée comme distance entre le cluster A et le cluster B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f40703-c6b0-4694-9ed7-041688c054d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXEMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6f42d9-10bc-4790-a397-5efd009621c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0684d1-75c2-46e5-849c-9669f7f595c7",
   "metadata": {},
   "source": [
    "# Using the AgglomerativeClustering() function with a distance threshold of 0 and no n_clusters, fit the dataset.\n",
    "clustering = AgglomerativeClustering(distance_threshold = 0, n_clusters = None).fit(viz)\n",
    "\n",
    "\n",
    "# Print out the distances of that model\n",
    "# I.e. n_clusters_, min(), and max()\n",
    "print(\"Distances du modèle :\")\n",
    "#La ligne print(\"Nombre de clusters :\", model.n_clusters_) affiche le nombre de clusters formés par le modèle.\n",
    "print(\"Nombre de clusters :\", clustering.n_clusters_)\n",
    "#La ligne print(\"Distance minimale :\", model.distances_.min()) affiche la distance minimale entre les clusters formés.\n",
    "print(\"Distance minimale :\", round(clustering.distances_.min(), 2))\n",
    "#La ligne print(\"Distance maximale :\", model.distances_.max()) affiche la distance maximale entre les clusters formés.\n",
    "print(\"Distance maximale :\", round(clustering.distances_.max(), 2))\n",
    "\n",
    "\n",
    "# Single\n",
    "# Using single linkage, create and display a dendogram.\n",
    "dendrogram = sch.dendrogram(sch.linkage(viz, method='single'))\n",
    "\n",
    "\n",
    "# Complete\n",
    "# Using complete linkage, create and display a dendogram.\n",
    "dendrogram = sch.dendrogram(sch.linkage(viz, method='complete'))\n",
    "\n",
    "\n",
    "# Average\n",
    "# Using average linkage, create and display a dendogram.\n",
    "dendrogram = sch.dendrogram(sch.linkage(viz, method='average'))\n",
    "\n",
    "\n",
    "# Centroid\n",
    "# Using centroid linkage, create and display a dendogram.\n",
    "dendrogram = sch.dendrogram(sch.linkage(viz, method='centroid'))\n",
    "\n",
    "\n",
    "# After observing the dendograms above, let's say that you want to choose \"Average\" linkage with only 2 clusters. \n",
    "# What are the labels of each of the datapoints?\n",
    "# Print these labels out\n",
    "\n",
    "agg_model = AgglomerativeClustering(n_clusters = 2, linkage ='average').fit(viz)\n",
    "\n",
    "cluster_label = agg_model.labels_\n",
    "print(cluster_label)\n",
    "\n",
    "for hitter, label in zip(viz.index, cluster_label):\n",
    "  print(f'Name of the hitter : {hitter} and their cluster {label}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
